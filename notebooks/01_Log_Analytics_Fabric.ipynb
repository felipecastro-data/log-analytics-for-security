{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6f0f79",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Log Analytics for Security\n",
    "\n",
    "This notebook processes and analyzes raw log files (Apache/Nginx-style) using PySpark. It detects patterns, flags suspicious behavior, and stores the results in Delta format for further analysis on Microsoft Fabric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aee268",
   "metadata": {},
   "source": [
    "### üß™ Step 1: Start Spark Session\n",
    "\n",
    "We begin by initializing a Spark session on Microsoft Fabric to process the log data using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa99e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Security Log Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # üëà Hide warnings like SparkUI port binding, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf68a3d",
   "metadata": {},
   "source": [
    "### üìÇ Step 2: Read Raw Log File from `/data/access.log`\n",
    "\n",
    "We load the Apache-style access log from the `data/` folder using Spark‚Äôs `read.text()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f06dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+\n",
      "|value                                                                               |\n",
      "+------------------------------------------------------------------------------------+\n",
      "|192.168.1.41 - - [10/May/2025:15:30:48 +0000] \"POST /api/data HTTP/1.1\" 403 2724    |\n",
      "|192.168.1.9 - - [10/May/2025:21:56:46 +0000] \"PUT /contact HTTP/1.1\" 200 766        |\n",
      "|192.168.1.13 - - [10/May/2025:13:35:18 +0000] \"POST /contact HTTP/1.1\" 503 3161     |\n",
      "|192.168.1.20 - - [10/May/2025:18:27:22 +0000] \"DELETE /index.html HTTP/1.1\" 302 1411|\n",
      "|192.168.1.24 - - [10/May/2025:19:00:41 +0000] \"GET /dashboard HTTP/1.1\" 503 2090    |\n",
      "+------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_file_path = \"../data/access.log\"  # Adjust the path for Fabric if needed\n",
    "\n",
    "raw_logs_df = spark.read.text(log_file_path)\n",
    "raw_logs_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c7682",
   "metadata": {},
   "source": [
    "### üîç Step 3: Parse Log Lines into Structured Format\n",
    "\n",
    "We apply regular expressions using `regexp_extract` to extract key fields from each log line:  \n",
    "- IP address  \n",
    "- Timestamp  \n",
    "- HTTP method  \n",
    "- Endpoint  \n",
    "- Protocol  \n",
    "- Status code  \n",
    "- Response size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fe34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------+------+-----------+--------+-----------+-------------+\n",
      "|ip          |timestamp                 |method|endpoint   |protocol|status_code|response_size|\n",
      "+------------+--------------------------+------+-----------+--------+-----------+-------------+\n",
      "|192.168.1.41|10/May/2025:15:30:48 +0000|POST  |/api/data  |HTTP/1.1|403        |2724         |\n",
      "|192.168.1.9 |10/May/2025:21:56:46 +0000|PUT   |/contact   |HTTP/1.1|200        |766          |\n",
      "|192.168.1.13|10/May/2025:13:35:18 +0000|POST  |/contact   |HTTP/1.1|503        |3161         |\n",
      "|192.168.1.20|10/May/2025:18:27:22 +0000|DELETE|/index.html|HTTP/1.1|302        |1411         |\n",
      "|192.168.1.24|10/May/2025:19:00:41 +0000|GET   |/dashboard |HTTP/1.1|503        |2090         |\n",
      "+------------+--------------------------+------+-----------+--------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "parsed_df = raw_logs_df.select(\n",
    "    regexp_extract('value', r'^(\\S+)', 1).alias('ip'),\n",
    "    regexp_extract('value', r'\\[(.*?)\\]', 1).alias('timestamp'),\n",
    "    regexp_extract('value', r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"', 1).alias('method'),\n",
    "    regexp_extract('value', r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"', 2).alias('endpoint'),\n",
    "    regexp_extract('value', r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"', 3).alias('protocol'),\n",
    "    regexp_extract('value', r'\\\"\\s(\\d{3})\\s', 1).cast(\"integer\").alias('status_code'),\n",
    "    regexp_extract('value', r'\\s(\\d+)$', 1).cast(\"integer\").alias('response_size')\n",
    ")\n",
    "\n",
    "parsed_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b024a9",
   "metadata": {},
   "source": [
    "### üßº Step 4: Clean and Filter\n",
    "\n",
    "We filter out records where parsing failed (null status codes) and inspect the resulting schema and sample rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227a86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status_code: integer (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n",
      "+------------+--------------------+------+-----------+--------+-----------+-------------+\n",
      "|          ip|           timestamp|method|   endpoint|protocol|status_code|response_size|\n",
      "+------------+--------------------+------+-----------+--------+-----------+-------------+\n",
      "|192.168.1.41|10/May/2025:15:30...|  POST|  /api/data|HTTP/1.1|        403|         2724|\n",
      "| 192.168.1.9|10/May/2025:21:56...|   PUT|   /contact|HTTP/1.1|        200|          766|\n",
      "|192.168.1.13|10/May/2025:13:35...|  POST|   /contact|HTTP/1.1|        503|         3161|\n",
      "|192.168.1.20|10/May/2025:18:27...|DELETE|/index.html|HTTP/1.1|        302|         1411|\n",
      "|192.168.1.24|10/May/2025:19:00...|   GET| /dashboard|HTTP/1.1|        503|         2090|\n",
      "+------------+--------------------+------+-----------+--------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = parsed_df.filter(col(\"status_code\").isNotNull())\n",
    "cleaned_df.printSchema()\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11450314",
   "metadata": {},
   "source": [
    "### üß† Step 5: Apply Basic Anomaly Detection\n",
    "\n",
    "We detect suspicious behavior by grouping logs by IP and identifying users with 3 or more requests resulting in HTTP status codes ‚â• 400.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76d40940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+\n",
      "|          ip|status_code|request_count|\n",
      "+------------+-----------+-------------+\n",
      "|192.168.1.40|        404|            2|\n",
      "|192.168.1.23|        404|            2|\n",
      "|192.168.1.35|        403|            2|\n",
      "|192.168.1.11|        404|            2|\n",
      "| 192.168.1.7|        401|            2|\n",
      "|192.168.1.14|        500|            2|\n",
      "+------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "anomalies_df = cleaned_df.groupBy(\"ip\", \"status_code\").agg(count(\"*\").alias(\"request_count\")) \\\n",
    "    .filter((col(\"status_code\") >= 400) & (col(\"request_count\") >= 2))\n",
    "\n",
    "anomalies_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10f22",
   "metadata": {},
   "source": [
    "### üíæ Step 6: Save Processed Logs to Delta Lake\n",
    "\n",
    "We store the cleaned log data as a Delta table in Microsoft Fabric‚Äôs default `Tables/` location for downstream analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbd24001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not write to Delta format. This step requires Microsoft Fabric or Delta Lake setup.\n",
      "\n",
      "Error details:\n",
      " An error occurred while calling o158.save.\n",
      ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.ClassNotFoundException: delta.DefaultSource\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
      "\tat scala.util.Failure.orElse(Try.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
      "\t... 16 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ü™£ Try saving to Delta (only works on Fabric or local Delta setup)\n",
    "try:\n",
    "    cleaned_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"Tables/security_logs_cleaned\")\n",
    "\n",
    "    print(\"‚úÖ Data written to Delta successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not write to Delta format. This step requires Microsoft Fabric or Delta Lake setup.\\n\")\n",
    "    print(\"Error details:\\n\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
